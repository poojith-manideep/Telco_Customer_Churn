{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72864cd",
   "metadata": {},
   "source": [
    "# Telco Customer Churn Prediction Assignment Problem Statement - 7\n",
    "\n",
    "#### Name: Yenduru Poojith Manideep\n",
    "#### BITS ID: 2024ac05966@wilp.bits-pilani.ac.in\n",
    "\n",
    "#### Objective: Predict customer churn for a telecommunications company using machine learning models\n",
    "\n",
    "## Section 1: Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb07938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import seaborn as sns  # For advanced visualization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setting visualization style\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b773f7ea",
   "metadata": {},
   "source": [
    "### Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Telco Customer Churn Dataset\n",
    "df = pd.read_csv('Telco-Dataset.csv')\n",
    "print('Dataset Loaded. Shape of the Dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32ecd2",
   "metadata": {},
   "source": [
    "## Section 2: Data Visualization and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)  # Show 10 rows for a richer preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73df62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset metadata & null scan\n",
    "print('Dataset shape:', df.shape)\n",
    "\n",
    "print('\\nColumn Types and Null Scan:')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68554ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "# df['TotalCharges'] = df['TotalCharges'].fillna(0)\n",
    "\n",
    "# print('\\nStatistical Description (Numerical Columns):')\n",
    "# display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb760ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Churn value counts to check balance\n",
    "# plt.figure(figsize=(4,3))\n",
    "# sns.countplot(data=df, x='Churn')\n",
    "# plt.title('Churn Class Distribution')\n",
    "# plt.show()\n",
    "\n",
    "# # Distribution of numerical features\n",
    "# num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "# fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "# for i, col in enumerate(num_cols):\n",
    "#     sns.histplot(df[col], bins=25, kde=True, ax=axs[i])\n",
    "#     axs[i].set_title(f'Distribution of {col}')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Distribution of categorical features\n",
    "# cat_cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "#             'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
    "\n",
    "# plt.figure(figsize=(16, 16))\n",
    "# for i, col in enumerate(cat_cols, 1):\n",
    "#     plt.subplot(4, 4, i)\n",
    "#     sns.countplot(x=col, data=df, color='steelblue')\n",
    "#     plt.tight_layout()\n",
    "# plt.suptitle('Categorical Feature Distributions', y=1.02, fontsize=16)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890713b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlational matrix (numerical)\n",
    "# plt.figure(figsize=(6,4))\n",
    "# corr = df[['tenure', 'MonthlyCharges']].corr(method='pearson')\n",
    "# sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "# plt.title('Correlation Matrix (Numerical Features)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4922724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.replace(['', ' ', 'NA', 'na', 'NaN', 'null', 'NULL'], np.nan)\n",
    "\n",
    "\n",
    "# Scan for missing values and datatypes\n",
    "print('Missing values per column:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df['TotalCharges'] = df['TotalCharges'].replace(' ', pd.NA)\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "print(f\"Missing TotalCharges before better imputation: {df['TotalCharges'].isna().sum()}\")\n",
    "\n",
    "\n",
    "# # Better imputation logic for missing values in TotalCharges\n",
    "\n",
    "# # For 0 tenure customers, set TotalCharges to MonthlyCharges\n",
    "zero_tenure_mask = (df['tenure'] == 0) & df['TotalCharges'].isna()\n",
    "df.loc[zero_tenure_mask, 'TotalCharges'] = df.loc[zero_tenure_mask, 'MonthlyCharges']\n",
    "\n",
    "# # For other missing values, estimate using tenure * MonthlyCharges  \n",
    "missing_mask = df['TotalCharges'].isna()\n",
    "df.loc[missing_mask, 'TotalCharges'] = df.loc[missing_mask, 'tenure'] * df.loc[missing_mask, 'MonthlyCharges']\n",
    "\n",
    "print(f\"Missing TotalCharges after better imputation: {df['TotalCharges'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual inspection for outliers\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18,4))\n",
    "for i, col in enumerate(['tenure', 'MonthlyCharges', 'TotalCharges']):\n",
    "    sns.boxplot(x=df[col], ax=axs[i], color='lightcoral')\n",
    "    axs[i].set_title(f'Boxplot - {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Skewness check\n",
    "skew_vals = df[['tenure','MonthlyCharges', 'TotalCharges']].skew()\n",
    "print('Skewness of Numeric Features:')\n",
    "print(skew_vals)\n",
    "\n",
    "# Why not immediately remove outliers? For customer churn, outliers are likely legitimate (very high tenure/charges are loyal customers), so removal may lose valuable business insight. We'll scale these instead rather than cap/drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffd5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop customerID\n",
    "df_ml = df.drop('customerID', axis=1)\n",
    "\n",
    "#Skewness of TotalCharges is  0.963316 - We need to reduce the skewness\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer as pt\n",
    "\n",
    "pt = pt(method=\"yeo-johnson\")\n",
    "df_ml['TotalCharges'] = pt.fit_transform(df_ml[[\"TotalCharges\"]])\n",
    "\n",
    "# Verify skewness after transformation\n",
    "print('Skewness after yeo-johnson on TotalCharges:', df_ml['TotalCharges'].skew())\n",
    "\n",
    "# Optional: Re-plot histogram for TotalCharges to visualize improvement\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_ml['TotalCharges'], kde=True)\n",
    "plt.title('Distribution of TotalCharges after log1p')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf5f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode redundant categories for internet-dependent features\n",
    "internet_dependent = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "for col in internet_dependent:\n",
    "    df_ml[col] = df_ml[col].replace('No internet service', 'No')\n",
    "\n",
    "# Similarly for MultipleLines\n",
    "df_ml['MultipleLines'] = df_ml['MultipleLines'].replace('No phone service', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Instantiate scaler\n",
    "scaler = StandardScaler()\n",
    "num_cols = ['tenure','MonthlyCharges','TotalCharges']\n",
    "df_ml[num_cols] = scaler.fit_transform(df_ml[num_cols])\n",
    "\n",
    "# Verify scaling\n",
    "df_ml[num_cols].describe().T[['mean','std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531dd641",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.head(10)  # Show 10 rows for a richer preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eaf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Engineering -> Creating new Features\n",
    "\n",
    "# Create new predictive features\n",
    "df_ml['AvgMonthlySpend'] = df_ml['TotalCharges'] / (df_ml['tenure'] + 1)\n",
    "df_ml['IsMonthToMonth'] = (df_ml['Contract'] == 'Month-to-month').astype(int)\n",
    "df_ml['IsElectronicCheck'] = (df_ml['PaymentMethod'] == 'Electronic check').astype(int)\n",
    "df_ml['HasPartnerOrDependents'] = ((df_ml['Partner'] == 'Yes') | (df_ml['Dependents'] == 'Yes')).astype(int)\n",
    "df_ml['HasPhoneAndInternet'] = ((df_ml['PhoneService'] == 'Yes') & (df_ml['InternetService'] != 'No')).astype(int)\n",
    "\n",
    "# Count of additional services\n",
    "service_cols = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "df_ml['AdditionalServices'] = sum([df_ml[col].map({'Yes': 1, 'No': 0}) for col in service_cols])\n",
    "\n",
    "# df_ml[\"tenure_group\"] = pd.cut(\n",
    "#     df_ml[\"tenure\"],\n",
    "#     bins=[0, 12, 24, 48, 60, 72],\n",
    "#     labels=[\"0-12\", \"12-24\", \"24-48\", \"48-60\", \"60-72\"],\n",
    "# )\n",
    "df_ml[\"is_long_contract\"] = df_ml[\"Contract\"].isin([\"One year\", \"Two year\"]).astype(int)\n",
    "df_ml[\"fiber_streaming\"] = (\n",
    "    (df_ml[\"InternetService\"] == \"Fiber optic\") & (df[\"StreamingTV\"] == \"Yes\")\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f86969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Instantiate scaler\n",
    "scaler = StandardScaler()\n",
    "num_cols = ['tenure','MonthlyCharges','TotalCharges', 'AvgMonthlySpend', 'IsMonthToMonth', 'AdditionalServices']\n",
    "df_ml[num_cols] = scaler.fit_transform(df_ml[num_cols])\n",
    "\n",
    "# Verify scaling\n",
    "df_ml[num_cols].describe().T[['mean','std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f70892",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode churn label\n",
    "df_ml['Churn'] = df_ml['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# List categorical features (excluding target)\n",
    "cat_feats = [col for col in df_ml.columns if df_ml[col].dtype == 'object' and col != 'Churn']\n",
    "\n",
    "# Show unique values for categorical features (documentation)\n",
    "for col in cat_feats:\n",
    "    print(f\"{col}: {df_ml[col].unique()}\")\n",
    "\n",
    "# Apply one-hot encoding (assign drop_first=False so all categories explicit)\n",
    "df_ml = pd.get_dummies(df_ml, columns=cat_feats, drop_first=False)\n",
    "\n",
    "print('Shape after encoding:', df_ml.shape)\n",
    "df_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a756d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "X = df_ml.drop('Churn', axis=1)\n",
    "y = df_ml['Churn']\n",
    "mi_scores = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "\n",
    "feat_importance = pd.Series(mi_scores, X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Top 15 features\n",
    "feat_importance[:15].plot(kind='barh', figsize=(8,6))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Feature Importance (Mutual Information w/ Churn)')\n",
    "plt.show()\n",
    "\n",
    "# Display top features (table)\n",
    "display(feat_importance.head(15).to_frame('MI Score'))\n",
    "\n",
    "# Drop low-importance features (MI <= 0.01, keep important only)\n",
    "selected_feats = feat_importance[feat_importance > 0.01].index.tolist()\n",
    "if 'Churn' not in selected_feats:\n",
    "    selected_feats.append('Churn')\n",
    "df_ml = df_ml[selected_feats]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ca38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Data for modeling\n",
    "X = df_ml.drop('Churn', axis=1)\n",
    "y = df_ml['Churn']\n",
    "\n",
    "# 80/20 split (default)\n",
    "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"80/20 split - Train: {X_train_80.shape}, Test: {X_test_20.shape}\")\n",
    "\n",
    "# 70/30 split\n",
    "X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(f\"70/30 split - Train: {X_train_70.shape}, Test: {X_test_30.shape}\")\n",
    "\n",
    "# 90/10 split\n",
    "X_train_90, X_test_10, y_train_90, y_test_10 = train_test_split(X, y, test_size=0.10, random_state=42, stratify=y)\n",
    "print(f\"90/10 split - Train: {X_train_90.shape}, Test: {X_test_10.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f7f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Churn distribution in full set:', y.mean().round(3))\n",
    "print('Churn in 80% train:', y_train_80.mean().round(3))\n",
    "print('Churn in 20% test:', y_test_20.mean().round(3))\n",
    "# Check other splits\n",
    "print('Churn in 70% train:', y_train_70.mean().round(3))\n",
    "print('Churn in 30% test:', y_test_30.mean().round(3))\n",
    "print('Churn in 90% train:', y_train_90.mean().round(3))\n",
    "print('Churn in 10% test:', y_test_10.mean().round(3))\n",
    "\n",
    "# Apply SMOTE to address class imbalance on training sets only\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "smote = SMOTEENN(random_state=42)\n",
    "\n",
    "X_train_80_bal, y_train_80_bal = smote.fit_resample(X_train_80, y_train_80)\n",
    "X_train_70_bal, y_train_70_bal = smote.fit_resample(X_train_70, y_train_70)\n",
    "X_train_90_bal, y_train_90_bal = smote.fit_resample(X_train_90, y_train_90)\n",
    "\n",
    "# X_train_80_bal, y_train_80_bal = X_train_80, y_train_80\n",
    "# X_train_70_bal, y_train_70_bal = X_train_70, y_train_70\n",
    "# X_train_90_bal, y_train_90_bal = X_train_90, y_train_90\n",
    "\n",
    "\n",
    "# Verify balance\n",
    "print('Balanced 80/20 train:', y_train_80_bal.mean().round(3))\n",
    "print('Balanced 70/30 train:', y_train_70_bal.mean().round(3))\n",
    "print('Balanced 90/10 train:', y_train_90_bal.mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=1000, class_weight='balanced')\n",
    "# Tune penalty and regularization strength\n",
    "param_grid_lr = {\n",
    "    'penalty': ['l1', 'l2'], \n",
    "    'C': [0.1, 0.5, 1, 2,3],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid search for optimal hyperparameters on 80/20 split\n",
    "gs_lr = GridSearchCV(logreg, param_grid_lr, cv=cv, scoring='f1', n_jobs=-1)\n",
    "gs_lr.fit(X_train_80_bal, y_train_80_bal)\n",
    "print(f\"Best LR params: {gs_lr.best_params_}; best CV ROC AUC: {gs_lr.best_score_:.3f}\")\n",
    "\n",
    "# Predict on test\n",
    "y_pred_lr = gs_lr.predict(X_test_20)\n",
    "y_proba_lr = gs_lr.predict_proba(X_test_20)[:,1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_20, y_pred_lr))\n",
    "print(\"Precision:\", precision_score(y_test_20, y_pred_lr))\n",
    "print(\"Recall:\", recall_score(y_test_20, y_pred_lr))\n",
    "print(\"F1 Score:\", f1_score(y_test_20, y_pred_lr))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test_20, y_proba_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "param_grid_dt = {\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "gs_dt = GridSearchCV(dt, param_grid_dt, cv=cv, scoring='f1', n_jobs=-1)\n",
    "gs_dt.fit(X_train_80_bal, y_train_80_bal)\n",
    "print(f\"Best Decision Tree params: {gs_dt.best_params_}; best CV ROC AUC: {gs_dt.best_score_:.3f}\")\n",
    "y_pred_dt = gs_dt.predict(X_test_20)\n",
    "y_proba_dt = gs_dt.predict_proba(X_test_20)[:,1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_20, y_pred_dt))\n",
    "print(\"Precision:\", precision_score(y_test_20, y_pred_dt))\n",
    "print(\"Recall:\", recall_score(y_test_20, y_pred_dt))\n",
    "print(\"F1 Score:\", f1_score(y_test_20, y_pred_dt))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test_20, y_proba_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18228f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 11, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "gs_knn = GridSearchCV(knn, param_grid_knn, cv=cv, scoring='f1', n_jobs=-1)\n",
    "gs_knn.fit(X_train_80_bal, y_train_80_bal)\n",
    "print(f\"Best KNN params: {gs_knn.best_params_}; best CV ROC AUC: {gs_knn.best_score_:.3f}\")\n",
    "y_pred_knn = gs_knn.predict(X_test_20)\n",
    "y_proba_knn = gs_knn.predict_proba(X_test_20)[:,1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_20, y_pred_knn))\n",
    "print(\"Precision:\", precision_score(y_test_20, y_pred_knn))\n",
    "print(\"Recall:\", recall_score(y_test_20, y_pred_knn))\n",
    "print(\"F1 Score:\", f1_score(y_test_20, y_pred_knn))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test_20, y_proba_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed4cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "gs_rf = GridSearchCV(rf, param_grid_rf, cv=cv, scoring='f1', n_jobs=-1)\n",
    "gs_rf.fit(X_train_80_bal, y_train_80_bal)\n",
    "print(f\"Best RF params: {gs_rf.best_params_}; best CV ROC AUC: {gs_rf.best_score_:.3f}\")\n",
    "y_pred_rf = gs_rf.predict(X_test_20)\n",
    "y_proba_rf = gs_rf.predict_proba(X_test_20)[:,1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_20, y_pred_rf))\n",
    "print(\"Precision:\", precision_score(y_test_20, y_pred_rf))\n",
    "print(\"Recall:\", recall_score(y_test_20, y_pred_rf))\n",
    "print(\"F1 Score:\", f1_score(y_test_20, y_pred_rf))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test_20, y_proba_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics for test set (80/20)\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "def eval_metrics(y_true, y_pred, y_proba, name):\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1': f1_score(y_true, y_pred),\n",
    "        'ROC_AUC': roc_auc_score(y_true, y_proba)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "results.append(eval_metrics(y_test_20, y_pred_lr, y_proba_lr, 'Logistic Regression'))\n",
    "results.append(eval_metrics(y_test_20, y_pred_dt, y_proba_dt, 'Decision Tree'))\n",
    "results.append(eval_metrics(y_test_20, y_pred_knn, y_proba_knn, 'KNN'))\n",
    "results.append(eval_metrics(y_test_20, y_pred_rf, y_proba_rf, 'Random Forest'))\n",
    "\n",
    "import pandas as pd\n",
    "perf_df = pd.DataFrame(results).set_index('Model')\n",
    "display(perf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance metrics for all models\n",
    "plt.figure(figsize=(10,6))\n",
    "perf_df[['Accuracy','Precision','Recall','F1','ROC_AUC']].plot(kind='bar', ax=plt.gca(), rot=15)\n",
    "plt.title('Comparison of Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0118930",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "models_pred = [\n",
    "    ('Logistic Regression', y_proba_lr),\n",
    "    ('Decision Tree', y_proba_dt),\n",
    "    ('KNN', y_proba_knn),\n",
    "    ('Random Forest', y_proba_rf)\n",
    "]\n",
    "for name, yproba in models_pred:\n",
    "    fpr, tpr, _ = roc_curve(y_test_20, yproba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC: {roc_auc_score(y_test_20, yproba):.2f})')\n",
    "plt.plot([0,1],[0,1],'k--', lw=0.8)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
