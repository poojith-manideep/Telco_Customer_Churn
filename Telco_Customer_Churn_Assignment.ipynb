{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72864cd",
   "metadata": {},
   "source": [
    "# Telco Customer Churn Prediction Assignment Problem Statement - 7\n",
    "\n",
    "#### Name: Yenduru Poojith Manideep\n",
    "#### BITS ID: 2024ac05966@wilp.bits-pilani.ac.in\n",
    "\n",
    "#### Objective: Predict customer churn for a telecommunications company using machine learning models\n",
    "\n",
    "---\n",
    "## Section 1: Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb07938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import seaborn as sns  # For advanced visualization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setting visualization style\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b773f7ea",
   "metadata": {},
   "source": [
    "### Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Telco Customer Churn Dataset\n",
    "df = pd.read_csv('Telco-Dataset.csv')\n",
    "print('Dataset Loaded. Shape of the Dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32ecd2",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Data Visualization and Exploration\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a critical step in any machine learning project. It helps build intuition about the data's structure, distributions, imbalances, correlations, and potential issues. Visualization makes patterns and outliers visible, while summary statistics and info reveal hidden or non-obvious data problems.\n",
    "\n",
    "### 2.a) Sanity Check: Preview Data\n",
    "Before doing any kind of modeling, I first take a quick look at the dataset. This is like a sanity check to see if the data is actually loaded properly and whether the columns and rows make sense. By just previewing the first few rows, I can immediately spot if there are weird values, missing headers, or any formatting issues. This step saves time later because if the data itself is broken, then the whole analysis will go wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)  # Show 10 rows for a richer preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c77ec",
   "metadata": {},
   "source": [
    "### 2.b) Describe and Check Dataset Details\n",
    "\n",
    "After the preview, I used .info() and .describe() to understand the data set better. This gives me details like the number of rows, columns, datatypes, null values, and also some basic statistics such as mean, min, max, and standard deviation. These details help me confirm if the columns are in the right type (like numbers should not be stored as strings), and also if there are any obvious errors in ranges or values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73df62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset metadata & null scan\n",
    "print('Dataset shape:', df.shape)\n",
    "\n",
    "print('\\nColumn Types and Null Scan:')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d55e6",
   "metadata": {},
   "source": [
    "### 2.c) Visual Exploration (Univariate and Bivariate Analysis)\n",
    "\n",
    "In univariate analysis, I study each column separately. This helps me see how the values are distributed—whether a feature is normally distributed, skewed, or has too many repeated values. For categorical columns, I check the frequency counts to see if the dataset is balanced or heavily imbalanced. This step is important because later on, the model’s performance depends on how well these features are understood and preprocessed.\n",
    "\n",
    "Observations :-\n",
    "\n",
    "1. For the numerical data, the distribution of Tenure and Monthly Charges is okay, but the TotalCharges is positively skewed. I am handling this later\n",
    "2. For the categorical data, we see a lot of class imbalance issue. I am handling this issue also later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb760ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn value counts to check balance\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.countplot(data=df, x='Churn')\n",
    "plt.title('Churn Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of numerical features\n",
    "num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(df[col], bins=25, kde=True, ax=axs[i])\n",
    "    axs[i].set_title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of categorical features\n",
    "cat_cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "            'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "for i, col in enumerate(cat_cols, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.countplot(x=col, data=df, color='steelblue')\n",
    "    plt.tight_layout()\n",
    "plt.suptitle('Categorical Feature Distributions', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b34a8",
   "metadata": {},
   "source": [
    "Bivariate analysis is about checking the relationship between two variables. Here, I usually compare the independent features with the target column (like churn vs tenure). This helps me see which features are actually related to the output. For example, if customers with shorter tenure are more likely to churn, then tenure is clearly an important feature. Doing this gives me early hints about which variables will be useful for the model.\n",
    "\n",
    "My Observation:-\n",
    "\n",
    "If we can see the first plot, Tenure vs Churn, we casn clearly see that the customers with shorter tenures are more likely got churned. SO Like this, for every Feature, I can get an insight of what would be the relation or correlation between each feature and Churn output class variable. So this is interesting for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# # Bivariate Analysis: Contract Type vs Churn\n",
    "total_columns = num_cols + cat_cols\n",
    "n_cols = 3   # number of plots per row (adjust if you want)\n",
    "n_rows = math.ceil(len(total_columns) / n_cols)\n",
    "\n",
    "plt.figure(figsize=(18, n_rows * 4))\n",
    "\n",
    "for i, col in enumerate(total_columns, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    \n",
    "    if col in num_cols:\n",
    "        sns.histplot(x=col, hue='Churn', data=df, palette='Set2', multiple='stack', bins=30)\n",
    "    else:\n",
    "        sns.countplot(x=col, hue='Churn', data=df, palette='Set2')\n",
    "    \n",
    "    plt.title(f'Churn Distribution by {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243cd8b4",
   "metadata": {},
   "source": [
    "### 2.d) Correlational Analysis\n",
    "\n",
    "**Numerical attribute correlation:** The correlation matrix shows how features are related to each other. If two features are highly correlated, then they are almost carrying the same information. Including both may confuse the model or add unnecessary noise. By checking the correlation matrix, I can identify redundancy and decide if I need to drop or combine some features. It also helps me see which variables are strongly correlated with the target (if included).\n",
    "\n",
    "The result of this analysis is a correlation coefficient, which typically ranges from -1 to +1.\n",
    "\n",
    "+1: Indicates a perfect positive linear relationship. As one variable increases, the other variable increases proportionally.\n",
    "\n",
    "-1: Indicates a perfect negative linear relationship. As one variable increases, the other variable decreases proportionally.\n",
    "\n",
    "0: Indicates no linear relationship between the variables.\n",
    "\n",
    "**Method That I have used:** I have used Pearson correlation matrix and computed on numerical features. This will be Visualized as a heatmap \n",
    "\n",
    "**Justification on Why should we use this:** Correlational analysis will be needed for the feature selection that will be performed in the next step.\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features to be used in building a machine learning model. The goal is to improve model performance, reduce complexity, and decrease training time. The correlation matrix is a primary tool for guiding this process.\n",
    "\n",
    "**Correlation between Independent Features (Features vs. Features)**\n",
    "\n",
    "The heatmap will reveal any pairs of independent variables if they are highly correlated with each other (e.g., a correlation coefficient of > 0.85 or < -0.85). This phenomenon is called multicollinearity.\n",
    "\n",
    "Effect on Feature Selection: If two features are highly correlated, they are essentially providing redundant information. Keeping both in the model can cause several problems. Therefore, the common practice is to remove one of the highly correlated features.\n",
    "\n",
    "Justification:\n",
    "\n",
    "Model Stability and Reliability: Multicollinearity can make the model's coefficient estimates unstable and highly sensitive to minor changes in the data. For linear models (like Linear Regression or Logistic Regression), it becomes difficult to determine the individual contribution of each correlated feature to the prediction. By removing one, the model becomes more stable and the coefficients become more interpretable.\n",
    "\n",
    "Reduces Redundancy and Complexity: The principle of Occam's Razor suggests that simpler models are better. If two features provide the same information, it is more efficient to use only one. This reduces the dimensionality of the data, which can lead to faster training times and a less complex, more generalizable model.\n",
    "\n",
    "Avoids Overfitting: Including redundant features can sometimes lead to the model fitting to the noise in the data rather than the underlying signal, which results in poor performance on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890713b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlational matrix (numerical)\n",
    "plt.figure(figsize=(6,4))\n",
    "corr = df[['tenure', 'MonthlyCharges']].corr(method='pearson')\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix (Numerical Features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86e2f0",
   "metadata": {},
   "source": [
    "Observation :- We only have 2 numerical features and Here both the features are not highly correlated with each other. So, There is no need to remove any feature. If any of them are highly correlated i.e., >0.85, then we should ideally have checked for correlation with output class and only keep one and remove other. In this case, there's no need to remove any of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a34f4f",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Data Pre-processing and Cleaning\n",
    "\n",
    "Careful pre-processing is crucial for classical machine learning models, as they have limited tolerance for data and encoding issues compared to deep learning. Here we address:\n",
    "- Missing/null values and type inconsistencies\n",
    "- Outlier and skew handling\n",
    "- Encoding categorical variables: binary, ordinal, nominal (one-hot), with justification for each feature\n",
    "- Feature scaling (Standardization/Normalization), with method selection rationale\n",
    "- Identifying important features for prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c8ed9",
   "metadata": {},
   "source": [
    "### 3.a) Handling Missing/NULL Values and Data Type Consistency\n",
    "Datasets usually have missing values, and we cannot just leave them blank because most machine learning models can’t handle null values. So, I am fill (imputing) them with proper logic—for example, replacing with mean/median for numerical values, or mode for categorical values. The idea is to fill missing values in such a way that I don’t distort the dataset. Sometimes, missing data itself may carry meaning (like no internet service → missing online backup), so I also use domain knowledge when imputing.\n",
    "\n",
    "- For this dataset, `TotalCharges` sometimes encodes missing values as blanks (''), so we must coerce and fix these. Other columns with `No internet service`/`No phone service` are valid categories, not missing values, so should be left as-is for categorical encoding.\n",
    "\n",
    "**Logic I have used for imputation** For the imputation in TotalCharges, I forst checked if the tenure is 0 and the TotalCharges has a missing value, then in that case, I simply populated the TotalCharges with MonthlyCharges because the tenure is 0\n",
    "\n",
    "In another case, I checked if the tenure is non zero, then I imputed the TotalCharges = tenure * MonthlyCharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4922724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.replace(['', ' ', 'NA', 'na', 'NaN', 'null', 'NULL'], np.nan)\n",
    "\n",
    "\n",
    "# Scan for missing values and datatypes\n",
    "print('Missing values per column:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df['TotalCharges'] = df['TotalCharges'].replace(' ', pd.NA)\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "print(f\"Missing TotalCharges before better imputation: {df['TotalCharges'].isna().sum()}\")\n",
    "\n",
    "\n",
    "# # Better imputation logic for missing values in TotalCharges\n",
    "\n",
    "# # For 0 tenure customers, set TotalCharges to MonthlyCharges\n",
    "zero_tenure_mask = (df['tenure'] == 0) & df['TotalCharges'].isna()\n",
    "df.loc[zero_tenure_mask, 'TotalCharges'] = df.loc[zero_tenure_mask, 'MonthlyCharges']\n",
    "\n",
    "# # For other missing values, estimate using tenure * MonthlyCharges  \n",
    "missing_mask = df['TotalCharges'].isna()\n",
    "df.loc[missing_mask, 'TotalCharges'] = df.loc[missing_mask, 'tenure'] * df.loc[missing_mask, 'MonthlyCharges']\n",
    "\n",
    "print(f\"Missing TotalCharges after better imputation: {df['TotalCharges'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d775f6b8",
   "metadata": {},
   "source": [
    "### 3.b) Outlier Handling\n",
    "\n",
    "Outliers are values that are far away from the normal range of data. If I don’t handle them, they can pull the mean and standard deviation in a wrong direction, and the model might become biased. By visually inspecting (like boxplots, scatterplots), I can quickly spot unusual values and decide whether to keep, transform, or remove them. Outliers are not always bad, but I need to check them to make sure they don’t disturb the training process.\n",
    "\n",
    "- For tenure and charges, I am using boxplots and statistical summaries to flag extreme values, then visually check distributions with log-transform or quantile capping if necessary (justified below).\n",
    "\n",
    "- Result :- I have not found any outliers for this data set. So, there is no need to do this step for this data set.\n",
    "\n",
    "We are lucky to not have outliers here BUT, For customer churn, outliers are likely legitimate (very high tenure/charges are loyal customers), so removal may lose valuable business insight. So In this case, not removing the outliers is a good choice as per design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual inspection for outliers\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18,4))\n",
    "for i, col in enumerate(['tenure', 'MonthlyCharges', 'TotalCharges']):\n",
    "    sns.boxplot(x=df[col], ax=axs[i], color='lightcoral')\n",
    "    axs[i].set_title(f'Boxplot - {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c008f",
   "metadata": {},
   "source": [
    "### 3.c) Skewness Detection\n",
    "\n",
    "Skewness tells me whether the data is symmetric or not. If a column is highly skewed, then many algorithms (like logistic regression, linear regression) may not perform well because they assume data is more normally distributed.\n",
    "\n",
    "In my case, the column ‘TotalCharges’ had many zero values, so I couldn’t apply log transformation (log of 0 is undefined). Instead, I used Yeo-Johnson transformation, which can handle zero and negative values as well. After transformation, the data becomes more balanced and closer to normal, which improves the model’s ability to learn patterns without being biased by extreme values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caea219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness check\n",
    "skew_vals = df[['tenure','MonthlyCharges', 'TotalCharges']].skew()\n",
    "print('Skewness of Numeric Features:')\n",
    "print(skew_vals)\n",
    "\n",
    "# Skewnesss of Tenure and MonthlyCharges are close to 0 -> So Transformation is not needed for them\n",
    "# Transformation is needed for TotalCharges feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c5ad7",
   "metadata": {},
   "source": [
    "I am dropping the customer Id from the dataset. Because, customer id is just an identifer and ther will be no logical relation with the output. So, removing that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffd5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop customerID\n",
    "df_ml = df.drop('customerID', axis=1)\n",
    "\n",
    "#Skewness of TotalCharges is  0.963316 - We need to reduce the skewness\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer as pt\n",
    "\n",
    "pt = pt(method=\"yeo-johnson\")\n",
    "df_ml['TotalCharges'] = pt.fit_transform(df_ml[[\"TotalCharges\"]])\n",
    "\n",
    "# Verify skewness after transformation\n",
    "print('Skewness after yeo-johnson on TotalCharges:', df_ml['TotalCharges'].skew())\n",
    "\n",
    "# Optional: Re-plot histogram for TotalCharges to visualize improvement\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_ml['TotalCharges'], kde=True)\n",
    "plt.title('Distribution of TotalCharges after log1p')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf887b9",
   "metadata": {},
   "source": [
    "Result :- After I have applied the yeo-johnson Transformation on TotalCharges, the skewness value is  -0.14 -> Which is close to 0. So, now all our attributes are not skewed and ready fot further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf5f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode redundant categories for internet-dependent features\n",
    "internet_dependent = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "for col in internet_dependent:\n",
    "    df_ml[col] = df_ml[col].replace('No internet service', 'No')\n",
    "\n",
    "# Similarly for MultipleLines\n",
    "df_ml['MultipleLines'] = df_ml['MultipleLines'].replace('No phone service', 'No')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e86bb",
   "metadata": {},
   "source": [
    "### 3.d) Feature Engineering\n",
    "\n",
    "Sometimes the raw dataset doesn’t directly capture useful information. Feature engineering is about creating new columns from the existing ones to improve the model. For example, combining tenure and Total charges might give a better signal about customer loyalty. By creating meaningful features, I can give the model more power to differentiate between classes. Basically, the quality of features often matters more than the complexity of the model. SO I have created various features here which are logically correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eaf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Engineering -> Creating new Features\n",
    "\n",
    "# Create new predictive features\n",
    "df_ml['AvgMonthlySpend'] = df_ml['TotalCharges'] / (df_ml['tenure'] + 1)\n",
    "df_ml['IsMonthToMonth'] = (df_ml['Contract'] == 'Month-to-month').astype(int)\n",
    "df_ml['IsElectronicCheck'] = (df_ml['PaymentMethod'] == 'Electronic check').astype(int)\n",
    "df_ml['HasPartnerOrDependents'] = ((df_ml['Partner'] == 'Yes') | (df_ml['Dependents'] == 'Yes')).astype(int)\n",
    "df_ml['HasPhoneAndInternet'] = ((df_ml['PhoneService'] == 'Yes') & (df_ml['InternetService'] != 'No')).astype(int)\n",
    "\n",
    "# Count of additional services\n",
    "service_cols = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "df_ml['AdditionalServices'] = sum([df_ml[col].map({'Yes': 1, 'No': 0}) for col in service_cols])\n",
    "\n",
    "\n",
    "df_ml[\"is_long_contract\"] = df_ml[\"Contract\"].isin([\"One year\", \"Two year\"]).astype(int)\n",
    "df_ml[\"fiber_streaming\"] = (\n",
    "    (df_ml[\"InternetService\"] == \"Fiber optic\") & (df[\"StreamingTV\"] == \"Yes\")\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d30f1",
   "metadata": {},
   "source": [
    "### 3.e) Feature Scaling -> Z Standardization\n",
    "\n",
    "**Why scale?** Classical ML models such as Logistic Regression (gradient-based), KNN (distance-based), and even tree ensembles (split-based, but still benefit) are sensitive to feature scale/variance. If numerical features vary on different scales, model could get biased or numerically unstable.\n",
    "\n",
    "Standard Scaler transforms the data so that each feature has mean = 0 and standard deviation = 1. This is important because in many ML algorithms (like logistic regression, SVM, neural networks), features with larger values can dominate the smaller ones. By scaling, all features are put on the same scale, which helps the model converge faster and improves performance. Without scaling, optimization may become unstable and lead to poor results.\n",
    "\n",
    "- **Which method to use?**\n",
    "    - **StandardScaler (mean 0, std 1):** Best for features with approximately bell-shaped (normal) distributions, and for algorithms making use of means/stds or distance (KNN, LR).\n",
    "    - **MinMaxScaler (0-1 scaling):** Used when features show non-Gaussian/skewed distributions. Keeps ranges between 0 and 1 (usually for algorithms relying on distance or bounded inputs).\n",
    "    - **RobustScaler:** For numeric features with a lot of outliers or heavy skew, scales by median and IQR efficiently.\n",
    "\n",
    "I am useing StandardScaler for 'tenure', 'MonthlyCharges', 'TotalCharges' as they are wide-range continuous and only mildly skewed. Here's the code for StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f86969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_cols = ['tenure','MonthlyCharges','TotalCharges', 'AvgMonthlySpend', 'AdditionalServices']\n",
    "df_ml[num_cols] = scaler.fit_transform(df_ml[num_cols])\n",
    "\n",
    "# Verify scaling\n",
    "df_ml[num_cols].describe().T[['mean','std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5842bd59",
   "metadata": {},
   "source": [
    "### 3.f) Encoding Categorical Features\n",
    "\n",
    "#### Justification:\n",
    "For classical ML, all input features must be numerical. But how we transform categorical variables greatly affects model results, interpretability, and risks of data leakage.\n",
    "\n",
    "**Encoding options include:**\n",
    "- **Label Encoding:** Assigns unique integer to each category. Good only for ordinal features (if order is meaningful) to preserve rank. *Problem:* Can create artificial order when applied to nominal features (e.g., gender).\n",
    "- **One-Hot Encoding:** Dummy variables for each category (except one as reference). Essential for truly nominal (unordered) features since it avoids false ordinality.\n",
    "- **Binary Encoding:** For high-cardinality features (many unique categories), it encodes categories as binary digits, reducing dimensionality compared to one-hot, but less interpretable. Not needed here due to low cardinality.\n",
    "\n",
    "#### Assignment of encoding method for each feature:\n",
    "| Feature             | Encoder     | Justification                                                      |\n",
    "|---------------------|-------------|--------------------------------------------------------------------|\n",
    "| gender              | Binary/One-hot      | Two categories, no order: one-hot preferred for LR/Tree/Ensemble |\n",
    "| SeniorCitizen       | Numeric (0/1) | Already numeric (0,1)                                             |\n",
    "| Partner, Dependents | Binary/One-hot      | Two categories, no order, one-hot to avoid LR bias              |\n",
    "| PhoneService        | Binary/One-hot      | Two categories, no order, one-hot                                |\n",
    "| MultipleLines       | One-hot      | 3 values: Yes/No/No phone service, one-hot preserves all info     |\n",
    "| InternetService     | One-hot      | 3 nominal values                                                  |\n",
    "| OnlineSecurity, OnlineBackup, DeviceProtection, etc | One-hot | 3 levels including 'No internet service', one-hot most robust |\n",
    "| Contract            | One-hot      | 3 unordered types                                                 |\n",
    "| PaperlessBilling    | Binary/One-hot      | Two values, no order, one-hot to avoid coefficients bias in LR  |\n",
    "| PaymentMethod       | One-hot      | 4 different methods, no order, one-hot for model reliability      |\n",
    "\n",
    "We avoid Label Encoding for non-ordinal variables to prevent giving unintended priority/order to categories, especially since Logistic Regression and tree methods handle one-hots natively.\n",
    "\n",
    "**Note:** I already dropped the customerID (non-predictive identifier) before modeling.\n",
    "\n",
    "**Churn** will be mapped to 1/0 for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f70892",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting feature Encoding\")\n",
    "\n",
    "# Encode churn label\n",
    "df_ml['Churn'] = df_ml['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "print(\"Churn -> Output class mapped to 0 and 1\")\n",
    "\n",
    "# List categorical features (excluding target)\n",
    "cat_feats = [col for col in df_ml.columns if df_ml[col].dtype == 'object' and col != 'Churn']\n",
    "\n",
    "# Show unique values for categorical features\n",
    "for col in cat_feats:\n",
    "    print(f\"{col}: {df_ml[col].unique()}\")\n",
    "\n",
    "# Apply one-hot encoding\n",
    "df_ml = pd.get_dummies(df_ml, columns=cat_feats, drop_first=False)\n",
    "\n",
    "print('Shape after encoding:', df_ml.shape)\n",
    "df_ml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b550f",
   "metadata": {},
   "source": [
    "### 3.g) Feature Importance Analysis for Engineering\n",
    "\n",
    "**Why important:** In classical ML, removing uninformative or redundant features improves model generalization and speeds up training. \n",
    "\n",
    "I an checking feature importance to understand which variables are contributing the most to predictions. This is useful because it shows whether the model is depending on meaningful features (like tenure, contract type) or just on noise. By ranking features, I also get ideas for feature engineering and for reducing less important columns that don’t add much value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a756d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "X = df_ml.drop('Churn', axis=1)\n",
    "y = df_ml['Churn']\n",
    "mi_scores = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "\n",
    "feat_importance = pd.Series(mi_scores, X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Top 15 features\n",
    "feat_importance[:15].plot(kind='barh', figsize=(8,6))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Feature Importance (Mutual Information w/ Churn)')\n",
    "plt.show()\n",
    "\n",
    "# Display top features (table)\n",
    "display(feat_importance.head(15).to_frame('MI Score'))\n",
    "\n",
    "# Drop low-importance features (MI <= 0.01, keep important only)\n",
    "selected_feats = feat_importance[feat_importance > 0.01].index.tolist()\n",
    "if 'Churn' not in selected_feats:\n",
    "    selected_feats.append('Churn')\n",
    "df_ml = df_ml[selected_feats]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd74c45",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Model Building \n",
    "\n",
    "### Overview:\n",
    "In this section, we develop and evaluate several classical machine learning models—Logistic Regression, Decision Tree, K-Nearest Neighbors, and an Ensemble (Random Forest)—to classify customer churn. We demonstrate the effect of different train/test splits and use cross-validation for hyperparameter tuning.\n",
    "\n",
    "\n",
    "#### 4.a) Train Test Split\n",
    "\n",
    "Splitting into train and test is very important. If I train and test on the same dataset, the model will just memorize the answers and show high accuracy, which is misleading. By splitting, I keep a part of the dataset hidden from training. Later, when I test, I can check whether the model can generalize to unseen data. This ensures that I am not overfitting and my performance results are genuine.\n",
    "\n",
    "- The standard split for model validation is 80% train, 20% test. This provides enough data for training while holding out a portion for an unbiased evaluation. However, smaller or larger splits (such as 70/30, 90/10) can be tested for sensitivity.\n",
    "- **80/20**: Widely accepted in industry; balances model learning and robust validation.\n",
    "- **70/30**: More data for validation; may benefit overfit-prone models.\n",
    "- **90/10**: More data for training; fewer for test—sometimes useful with smaller datasets or when model learning is challenging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ca38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Data for modeling\n",
    "X = df_ml.drop('Churn', axis=1)\n",
    "y = df_ml['Churn']\n",
    "\n",
    "# 80/20 split (default)\n",
    "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"80/20 split - Train: {X_train_80.shape}, Test: {X_test_20.shape}\")\n",
    "\n",
    "# 70/30 split\n",
    "X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(f\"70/30 split - Train: {X_train_70.shape}, Test: {X_test_30.shape}\")\n",
    "\n",
    "# 90/10 split\n",
    "X_train_90, X_test_10, y_train_90, y_test_10 = train_test_split(X, y, test_size=0.10, random_state=42, stratify=y)\n",
    "print(f\"90/10 split - Train: {X_train_90.shape}, Test: {X_test_10.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f5d805",
   "metadata": {},
   "source": [
    "#### 4.b) Addressing the Class Imbalance problem using SMOTE - Synthetic Minority Oversampling Technique\n",
    "\n",
    "In my dataset, the classes are imbalanced (very few churn cases compared to non-churn). If I don’t fix this, the model will always predict the majority class and still show high accuracy, but it will fail to capture churners.\n",
    "\n",
    "So, I applied SMOTE (Synthetic Minority Oversampling Technique) to generate new synthetic examples of the minority class. After that, I also used SMOTEENN (combination of SMOTE + Edited Nearest Neighbors) which balances the data better by removing overlapping/noisy samples. This way, the model learns to give attention to both classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f7f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Churn distribution in full set:', y.mean().round(3))\n",
    "print('Churn in 80% train:', y_train_80.mean().round(3))\n",
    "print('Churn in 20% test:', y_test_20.mean().round(3))\n",
    "# Check other splits\n",
    "print('Churn in 70% train:', y_train_70.mean().round(3))\n",
    "print('Churn in 30% test:', y_test_30.mean().round(3))\n",
    "print('Churn in 90% train:', y_train_90.mean().round(3))\n",
    "print('Churn in 10% test:', y_test_10.mean().round(3))\n",
    "\n",
    "# Applying SMOTE to address class imbalance on training sets only\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "smote = SMOTEENN(random_state=42)\n",
    "\n",
    "X_train_80_bal, y_train_80_bal = smote.fit_resample(X_train_80, y_train_80)\n",
    "X_train_70_bal, y_train_70_bal = smote.fit_resample(X_train_70, y_train_70)\n",
    "X_train_90_bal, y_train_90_bal = smote.fit_resample(X_train_90, y_train_90)\n",
    "\n",
    "\n",
    "# Verify balance\n",
    "print('Balanced 80/20 train:', y_train_80_bal.mean().round(3))\n",
    "print('Balanced 70/30 train:', y_train_70_bal.mean().round(3))\n",
    "print('Balanced 90/10 train:', y_train_90_bal.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627bc8be",
   "metadata": {},
   "source": [
    "## _Important NOTE_\n",
    "I first did the train-test split and then applied SMOTE only on the training set. After researching on this, I have found that This is the correct and recommended practice because applying SMOTE before splitting would allow synthetic data to leak into the test set, which gives unrealistically high results.\n",
    "\n",
    "In fact, when I tried the reverse way i.e., Applying SMOTE first and then splitting the data, I was getting accuracy and F1 scores around 98%, which is clearly misleading. So, I am consciously following the best practice here to make sure my evaluation is fair and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7cf67",
   "metadata": {},
   "source": [
    "#### 4.c) Logistic Regression + Hyperparameter Tuning\n",
    "\n",
    "I tuned Logistic Regression using GridSearchCV. The parameters I experimented with are:\n",
    "\n",
    "Penalty (l1, l2) → decides how regularization is applied (to prevent overfitting).\n",
    "\n",
    "C (0.1, 0.5, 1, 2, 3) → inverse of regularization strength. Smaller C means stronger regularization and higher penalty in case of oversampling.\n",
    "\n",
    "class_weight (balanced / none) → helps handle imbalance.\n",
    "\n",
    "GridSearchCV helps me test different combinations automatically and pick the best set. Logistic regression is simple, interpretable, and gives me a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=1000, class_weight='balanced')\n",
    "# Tune penalty and regularization strength\n",
    "param_grid_lr = {\n",
    "    'penalty': ['l1', 'l2'], \n",
    "    'C': [0.1, 0.5, 1, 2, 3],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid search for optimal hyperparameters on 80/20 split\n",
    "gs_lr = GridSearchCV(logreg, param_grid_lr, cv=cv, scoring='f1', n_jobs=-1)\n",
    "gs_lr.fit(X_train_80_bal, y_train_80_bal)\n",
    "print(f\"Best LR params: {gs_lr.best_params_}; best CV ROC AUC: {gs_lr.best_score_:.3f}\")\n",
    "\n",
    "# Predict on test\n",
    "y_pred_lr = gs_lr.predict(X_test_20)\n",
    "y_proba_lr = gs_lr.predict_proba(X_test_20)[:,1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_20, y_pred_lr))\n",
    "print(\"Precision:\", precision_score(y_test_20, y_pred_lr))\n",
    "print(\"Recall:\", recall_score(y_test_20, y_pred_lr))\n",
    "print(\"F1 Score:\", f1_score(y_test_20, y_pred_lr))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test_20, y_proba_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c1665",
   "metadata": {},
   "source": [
    "#### 4.d) Decision Tree Classifier + Hyperparameter Tuning\n",
    "\n",
    "- It is very much Flexible, interpretable for business rules by using the Decision Trees\n",
    "- Hyperparameters control overfitting (max_depth, min_samples_split).\n",
    "\n",
    "For Decision Trees, I tuned the following:\n",
    "\n",
    "max_depth (10, 15, 20, 30) → controls how deep the tree can go. Prevents overfitting.\n",
    "\n",
    "min_samples_split (2, 5, 10, 20) → minimum samples required to split a node.\n",
    "\n",
    "min_samples_leaf (1, 2, 5) → ensures leaves have enough data.\n",
    "\n",
    "class_weight (balanced / none) → to deal with imbalance.\n",
    "\n",
    "##### Justification: Decision Tree Tuning\n",
    "- `max_depth` prevents overly complex trees (risk of overfit).\n",
    "- `min_samples_split` prevents splits on tiny samples that overfit noise.\n",
    "- Values are chosen to allow both shallow and deep trees for comparison.\n",
    "\n",
    "Decision Trees are good because they can capture non-linear relationships, but they can overfit, so tuning is very important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "param_grid_dt = {\n",
    "    'max_depth': [10, 15, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "gs_dt = GridSearchCV(dt, param_grid_dt, cv=cv, scoring='f1', n_jobs=-1)\n",
    "gs_dt.fit(X_train_80_bal, y_train_80_bal)\n",
    "print(f\"Best Decision Tree params: {gs_dt.best_params_}; best CV ROC AUC: {gs_dt.best_score_:.3f}\")\n",
    "y_pred_dt = gs_dt.predict(X_test_20)\n",
    "y_proba_dt = gs_dt.predict_proba(X_test_20)[:,1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_20, y_pred_dt))\n",
    "print(\"Precision:\", precision_score(y_test_20, y_pred_dt))\n",
    "print(\"Recall:\", recall_score(y_test_20, y_pred_dt))\n",
    "print(\"F1 Score:\", f1_score(y_test_20, y_pred_dt))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test_20, y_proba_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e5428",
   "metadata": {},
   "source": [
    "#### 4.e) K-Nearest Neighbors (KNN) + Hyperparameter Tuning\n",
    "\n",
    "For KNN, I tuned:\n",
    "\n",
    "n_neighbors (1, 3, 5) → number of nearest neighbors considered.\n",
    "\n",
    "weights (uniform, distance) → whether all neighbors are equal or closer ones are weighted more.\n",
    "\n",
    "metric (euclidean, manhattan) → distance calculation method.\n",
    "\n",
    "KNN is simple but sensitive to scaling and outliers, so I used StandardScaler before applying it.\n",
    "\n",
    "##### Justification: KNN Tuning\n",
    "- `n_neighbors`: balances bias-variance (small = highly flexible, large = smoother)\n",
    "- `weights`: uniform for classic, distance for soft-voting\n",
    "- `metric`: explores different distance measures (Minkowski covers Euclidean and Manhattan)\n",
    "- Scaling/standardization is especially crucial, handled above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18228f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [1, 3, 5],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "gs_knn = GridSearchCV(knn, param_grid_knn, cv=cv, scoring='f1', n_jobs=-1)\n",
    "gs_knn.fit(X_train_80_bal, y_train_80_bal)\n",
    "print(f\"Best KNN params: {gs_knn.best_params_}; best CV ROC AUC: {gs_knn.best_score_:.3f}\")\n",
    "y_pred_knn = gs_knn.predict(X_test_20)\n",
    "y_proba_knn = gs_knn.predict_proba(X_test_20)[:,1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_20, y_pred_knn))\n",
    "print(\"Precision:\", precision_score(y_test_20, y_pred_knn))\n",
    "print(\"Recall:\", recall_score(y_test_20, y_pred_knn))\n",
    "print(\"F1 Score:\", f1_score(y_test_20, y_pred_knn))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test_20, y_proba_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9683f8",
   "metadata": {},
   "source": [
    "#### 4.e) Random Forest Classifier + Hyperparameter Tuning\n",
    "\n",
    "- Robust, handles both categorical and numerical, reduces variance.\n",
    "- Primary tuning: number of trees (n_estimators), max_depth.\n",
    "\n",
    "For Random Forest, I tuned:\n",
    "\n",
    "n_estimators (100, 200, 300) → number of trees in the forest.\n",
    "\n",
    "max_depth (10, 15, None) → limit of tree depth.\n",
    "\n",
    "min_samples_split (2, 5, 10) → avoids too fine splits.\n",
    "\n",
    "class_weight (balanced / none).\n",
    "\n",
    "Random Forests are powerful because they average results across many trees, reducing overfitting and improving stability.\n",
    "\n",
    "##### Justification: Random Forest Tuning\n",
    "- `n_estimators`: More trees = greater stability, but with diminishing returns after around 100-200.\n",
    "- `max_depth`: Controls tree size/complexity. None = unpruned.\n",
    "- `min_samples_split`: Prevents overfitting to small branches.\n",
    "- RF is robust to uninformative features due to built-in selection, so we use the whole set post-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed4cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "gs_rf = GridSearchCV(rf, param_grid_rf, cv=cv, scoring='f1', n_jobs=-1)\n",
    "gs_rf.fit(X_train_80_bal, y_train_80_bal)\n",
    "print(f\"Best RF params: {gs_rf.best_params_}; best CV ROC AUC: {gs_rf.best_score_:.3f}\")\n",
    "y_pred_rf = gs_rf.predict(X_test_20)\n",
    "y_proba_rf = gs_rf.predict_proba(X_test_20)[:,1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_20, y_pred_rf))\n",
    "print(\"Precision:\", precision_score(y_test_20, y_pred_rf))\n",
    "print(\"Recall:\", recall_score(y_test_20, y_pred_rf))\n",
    "print(\"F1 Score:\", f1_score(y_test_20, y_pred_rf))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test_20, y_proba_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c7b8ba",
   "metadata": {},
   "source": [
    "#### 4.f) Ensemble Learning – Why I Used It\n",
    "\n",
    "I used an ensemble of: XGBoost Classifier (xgb_clf) + LightGBM Classifier (lgbm_clf) + Random Forest Classifier (rf_clf)\n",
    "\n",
    "Ensemble models combine the strengths of multiple algorithms. For example, XGBoost and LightGBM are boosting algorithms that focus on misclassified samples, while Random Forest is a bagging method that reduces variance. Using them together gave me stronger results compared to individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# Define base models\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "lgbm_clf = LGBMClassifier(random_state=42)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create voting classifier (hard voting = majority vote)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_clf),\n",
    "        ('lgbm', lgbm_clf),\n",
    "        ('rf', rf_clf)\n",
    "    ],\n",
    "    voting='soft'  # Use 'soft' if you want to average predicted probabilities\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "voting_clf.fit(X_train_80_bal, y_train_80_bal)\n",
    "y_pred_ensemble = voting_clf.predict(X_test_20)\n",
    "y_proba_ensemble = voting_clf.predict_proba(X_test_20)[:,1]\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "print(\"Voting Classifier Accuracy:\", accuracy_score(y_test_20, y_pred_ensemble))\n",
    "print(\"Voting Classifier Precision:\", precision_score(y_test_20, y_pred_ensemble))\n",
    "print(\"Voting Classifier Recall:\", recall_score(y_test_20, y_pred_ensemble))\n",
    "print(\"Voting Classifier F1 Score:\", f1_score(y_test_20, y_pred_ensemble))\n",
    "print(\"Voting Classifier ROC AUC:\", roc_auc_score(y_test_20, y_proba_ensemble))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d95f794",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning Results + Precision vs Recall\n",
    "\n",
    "Across all models, I observed that:\n",
    "\n",
    "Recall was high (~82%) → meaning the model is catching most of the churn cases.\n",
    "\n",
    "Precision was low (~50%) → meaning many of the predicted churns were actually false alarms.\n",
    "\n",
    "This trade-off happened because of the dataset itself—class imbalance + overlapping feature values made it harder to get high precision. Since recall is more important in churn prediction (we don’t want to miss churners), I accepted this trade-off, but noted that precision kept the F1 score around 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe60a21",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Performance Evaluation & Selection of the Best Model\n",
    "\n",
    "Now we will evaluate all models using classical metrics for binary classification—**Accuracy, Precision, Recall, F1-Score**, and **ROC-AUC**—on the held-out test set.\n",
    "\n",
    "### Why these metrics?\n",
    "- **Accuracy:** Simple overall correctness; less reliable for imbalanced classes.\n",
    "- **Precision/Recall/F1:** Handle class imbalance, reflect business concern about false approvals (precision) and missed at-risk customers (recall).\n",
    "- **ROC-AUC:** Comprehensive—measures model's ability to rank churners above non-churners regardless of threshold; best for business action ranking.\n",
    "- **Justification:** Using all together ensures robust selection; ROC-AUC is often considered most reliable for churn, F1 balances miss and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics for test set (80/20)\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "def eval_metrics(y_true, y_pred, y_proba, name):\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1': f1_score(y_true, y_pred),\n",
    "        'ROC_AUC': roc_auc_score(y_true, y_proba)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "results.append(eval_metrics(y_test_20, y_pred_lr, y_proba_lr, 'Logistic Regression'))\n",
    "results.append(eval_metrics(y_test_20, y_pred_dt, y_proba_dt, 'Decision Tree'))\n",
    "results.append(eval_metrics(y_test_20, y_pred_knn, y_proba_knn, 'KNN'))\n",
    "results.append(eval_metrics(y_test_20, y_pred_rf, y_proba_rf, 'Random Forest'))\n",
    "results.append(eval_metrics(y_test_20, y_pred_ensemble, y_proba_ensemble, 'Voting Classifier[Ensemble]'))\n",
    "\n",
    "\n",
    "perf_df = pd.DataFrame(results).set_index('Model')\n",
    "display(perf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae1dd5",
   "metadata": {},
   "source": [
    "## Visualization: Comparison of Evaluation Metrics Across Models\n",
    "\n",
    "**Justification:**\n",
    "A bar/chart plot allows for immediate visual benchmarking across all performance metrics for each model. This is the standard for model comparison and helps communicate results to technical and non-technical stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance metrics for all models\n",
    "plt.figure(figsize=(10,6))\n",
    "perf_df[['Accuracy','Precision','Recall','F1','ROC_AUC']].plot(kind='bar', ax=plt.gca(), rot=15)\n",
    "plt.title('Comparison of Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fba40c",
   "metadata": {},
   "source": [
    "## ROC Curves for All Models\n",
    "\n",
    "Visualizing ROC-AUC curves side by side gives a nuanced view of true positive/false positive trade-offs and allows comparison at every threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0118930",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "models_pred = [\n",
    "    ('Logistic Regression', y_proba_lr),\n",
    "    ('Decision Tree', y_proba_dt),\n",
    "    ('KNN', y_proba_knn),\n",
    "    ('Random Forest', y_proba_rf),\n",
    "    ('Voting Classifier[Ensemble]', y_proba_ensemble)\n",
    "]\n",
    "for name, yproba in models_pred:\n",
    "    fpr, tpr, _ = roc_curve(y_test_20, yproba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC: {roc_auc_score(y_test_20, yproba):.2f})')\n",
    "plt.plot([0,1],[0,1],'k--', lw=0.8)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f460a10d",
   "metadata": {},
   "source": [
    "### Which Model is Best? (and Why)\n",
    "\n",
    "**Best Model Selection (with Justification)**\n",
    "\n",
    "- Model selection should be based primarily on ROC-AUC and F1-score, as they provide robust indicators of balance between generalization and actionable predictive value on imbalanced classification problems like churn.\n",
    "- From the metrics and ROC curve above, the model with the highest AUC and F1, while maintaining strong precision/recall, should be chosen. In customer churn, ROC-AUC >0.84 is considered highly effective.\n",
    "\n",
    "**Justification:**\n",
    "- **Random Forest** usually outperforms others here, due to its ability to model nonlinear relationships and handle interactions between features robustly and without the need for explicit transformation or pruning.\n",
    "- Logistic Regression offers interpretability, but is outperformed on nonlinear interaction data.\n",
    "- Decision Tree is interpretable but often overfits or underfits compared to ensembles, as shown by lower test AUC/F1.\n",
    "- KNN, while useful for simple separation, performs worse on mixed/complex categorical+numeric features and high dimensionality after encoding.\n",
    "\n",
    "**Conclusion:**\n",
    "> _The best classical machine learning model for this Telco Customer Churn prediction task is the **Random Forest classifier**, as evidenced by the highest ROC-AUC, F1, and balanced metrics in both visual and tabular comparisons above._\n",
    "\n",
    "Random Forest should be deployed for customer churn scoring but logistic regression coefficients may still be referenced for business explainability of individual attributes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
