{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72864cd",
   "metadata": {},
   "source": [
    "# Telco Customer Churn Prediction Assignment Problem Statement - 7\n",
    "\n",
    "#### Name: Yenduru Poojith Manideep\n",
    "#### BITS ID: 2024ac05966@wilp.bits-pilani.ac.in\n",
    "\n",
    "#### Objective: Predict customer churn for a telecommunications company using machine learning models\n",
    "\n",
    "## Section 1: Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb07938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import numpy as np   # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import seaborn as sns  # For advanced visualization\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import PowerTransformer as pt\n",
    "\n",
    "\n",
    "# Setting visualization style\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b773f7ea",
   "metadata": {},
   "source": [
    "### Load the Dataset\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "Loading the raw CSV using `pandas.read_csv` allows us to efficiently parse and inspect the data. Pandas makes it easy to spot data types, missing values, and enables immediate profiling for downstream preprocessing and analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Telco Customer Churn Dataset\n",
    "df = pd.read_csv('Telco-Dataset.csv')\n",
    "print('Dataset Loaded. Shape of the Dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32ecd2",
   "metadata": {},
   "source": [
    "## Section 2: Data Visualization and Exploration\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a critical step in any machine learning project. It helps build intuition about the data's structure, distributions, imbalances, correlations, and potential issues. Visualization makes patterns and outliers visible, while summary statistics and info reveal hidden or non-obvious data problems.\n",
    "\n",
    "#### 2.a) Sanity Check: Preview Data\n",
    "**Justification:**\n",
    "Previewing the first few rows ensures the dataset loaded as expected—columns, data types, and target variable are intact. This helps spot major import/formatting errors instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)  # Show 10 rows for a richer preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca37e2bf",
   "metadata": {},
   "source": [
    "#### 2.b) Describe and Check Dataset Details\n",
    "\n",
    "- **Metadata:**\n",
    "    - Number of rows and columns\n",
    "    - Datatype of each variable\n",
    "    - Count of missing/null values (initial scan)\n",
    "    - Statistical summary of numerical columns\n",
    "- **Justification:**\n",
    "    - `.info()` and `.shape` confirm both variable types and missingness.\n",
    "    - `.describe()` summarizes numerical variables, including any anomalies (negative values, extreme outliers, or constant columns)\n",
    "\n",
    "While there are other profiling tools (like `pandas_profiling` or `dtale`), these are preferred for transparency and assignment reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73df62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset metadata & null scan\n",
    "print('Dataset shape:', df.shape)\n",
    "\n",
    "print('\\nColumn Types and Null Scan:')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb373f5",
   "metadata": {},
   "source": [
    "I am converting TotalCharges from object to numeric (float), as it's actually a numerical data. \n",
    "\n",
    "We are doing this before itself because we need to do correlation analysis for numerical data in EDA step itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68554ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(0)\n",
    "\n",
    "print('\\nStatistical Description (Numerical Columns):')\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a80b65",
   "metadata": {},
   "source": [
    "#### 2.c) Visual Exploration (Univariate and Bivariate Analysis)\n",
    "\n",
    "We'll visualize both categorical and numerical variables to check distributions, imbalances, and relationships.\n",
    "\n",
    "**Univariate Analysis:** For each variable, visualize its distribution/frequency.\n",
    "- **Justification:**\n",
    "    - `countplot` is most effective for categorical (showing counts/imbalances, e.g., Churn)\n",
    "    - `histplot` (with `kde`) for numerical (to check for skew, outliers, or multimodal distributions)\n",
    "    - Alternatives: `boxplot` or `violinplot` (good for spotting outliers, but less direct for proportions). Histograms allow density shapes to appear clearly and are preferred for initial numeric variable review.\n",
    "\n",
    "**Bivariate Analysis:** Relationship between features (categorical/numeric) and target (`Churn`).\n",
    "- **Justification:**\n",
    "    - For each feature, seeing how distributions change across the churn target can highlight strong predictors/imbalances or the need for grouping/binarization.\n",
    "\n",
    "These plots form the basis for later feature transformation and selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb760ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn value counts to check balance\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.countplot(data=df, x='Churn')\n",
    "plt.title('Churn Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of numerical features\n",
    "num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "for i, col in enumerate(num_cols):\n",
    "    sns.histplot(df[col], bins=25, kde=True, ax=axs[i])\n",
    "    axs[i].set_title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of categorical features\n",
    "cat_cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "            'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "for i, col in enumerate(cat_cols, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.countplot(x=col, data=df, color='steelblue')\n",
    "    plt.tight_layout()\n",
    "plt.suptitle('Categorical Feature Distributions', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ed510",
   "metadata": {},
   "source": [
    "#### 2.d) Correlational Analysis\n",
    "- **Numerical attribute correlation:** Useful to spot redundancy and strong links which may impact model multicollinearity or suggest engineered features.\n",
    "- **Method:** Pearson correlation matrix is computed only on numerical features. Visualized as a heatmap (easier to see patterns than numbers).\n",
    "- **Justification:**\n",
    "    - **Why Pearson?** For continuous variables it’s most common and interpretable. Alternatives such as Spearman or Kendall available for ordinal/monotonic relations but here not required at first.\n",
    "    - **Limitations:** This matrix ignores categorical predictors (which might relate strongly to churn—next steps will address that using mutual information/chi-square, etc). For classical ML, analyzing both correlation *and* categorical-target relationships is best practice for feature selection.\n",
    "    - **Alternative techniques:** For datasets with many categorical features (like this one), we could use Cramer's V, mutual information, or pairwise Chi-square for categorical features, but correlation heatmap is a required industry baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890713b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlational matrix (numerical)\n",
    "plt.figure(figsize=(6,4))\n",
    "corr = df[['tenure', 'MonthlyCharges', 'TotalCharges']].corr(method='pearson')\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix (Numerical Features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd287ba",
   "metadata": {},
   "source": [
    "##### Justification: Impact of Correlation Analysis on Feature Selection\n",
    "\n",
    "**Will this affect feature selection?**\n",
    "\n",
    "- Correlational analysis of *numerical* features helps identify redundant or highly correlated (multicollinear) variables. In linear models (like Logistic Regression), high correlation between predictors can undermine interpretability and model stability, so one of two highly correlated features may be dropped. However, tree-based and other nonlinear models are less sensitive.\n",
    "- However, most features in this dataset are **categorical**; Pearson's r ignores these. To assess feature importance for categorical columns, one should use chi-square, mutual information, or Cramér's V as supplement.\n",
    "- **Therefore:** While correlation heatmaps guide *numeric* feature selection, *categorical* feature selection requires additional measures. For full rigor, we should supplement correlation matrices with information-theoretic/association analyses for categorical features in later steps. This multi-pronged approach maximizes model reliability and explanatory power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee82ca",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Data Pre-processing and Cleaning\n",
    "\n",
    "Careful pre-processing is crucial for classical machine learning models, as they have limited tolerance for data and encoding issues compared to deep learning. Here we address:\n",
    "- Missing/null values and type inconsistencies\n",
    "- Outlier and skew handling\n",
    "- Encoding categorical variables: binary, ordinal, nominal (one-hot), with justification for each feature\n",
    "- Feature scaling (Standardization/Normalization), with method selection rationale\n",
    "- Identifying important features for prediction\n",
    "\n",
    "## 3.a) Handling Missing/NULL Values and Data Type Consistency\n",
    "#### Justification:\n",
    "- Classical ML models like Logistic Regression, Decision Tree, KNN all require true numerical input; string/object types and NaNs must be fixed (impute or drop, but justify when dropping is acceptable vs business impact).\n",
    "- For this dataset, `TotalCharges` sometimes encodes missing values as blanks (''), so we must coerce and fix these. Other columns with `No internet service`/`No phone service` are valid categories, not missing values, so should be left as-is for categorical encoding.\n",
    "- Missing in categorical: impute with mode for non-informative missing; for numerical, mean/median depending on skew (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4922724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan for missing values and datatypes\n",
    "print('Missing values per column:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Special handling: Convert 'TotalCharges' to numeric; blanks to NaN\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Now, recalculate missing after conversion\n",
    "print('Missing after TotalCharges coercion:', df['TotalCharges'].isnull().sum())\n",
    "# Show rows with missing TotalCharges for manual inspection\n",
    "display(df[df['TotalCharges'].isnull()])\n",
    "\n",
    "# Safe to impute these as zero tenure means new customers with no charges\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c91dd0",
   "metadata": {},
   "source": [
    "## 3.b) Outlier and Skewness Detection & Handling\n",
    "#### Justification:\n",
    "- Outliers can bias classical models, especially those using linear decision boundaries (e.g., Logistic Regression, KNN with distance).\n",
    "- For tenure and charges, we'll use boxplots and statistical summaries to flag extreme values, then visually check distributions with log-transform or quantile capping if necessary (justified below).\n",
    "- Feature distribution skewness matters for models assuming feature Gaussianity (e.g., KNN, Logistic Regression, some distance/variance-based metrics). We'll check for strong skew in numeric columns: if present, log transformation or scaling can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual inspection for outliers\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18,4))\n",
    "for i, col in enumerate(['tenure', 'MonthlyCharges', 'TotalCharges']):\n",
    "    sns.boxplot(x=df[col], ax=axs[i], color='lightcoral')\n",
    "    axs[i].set_title(f'Boxplot - {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Skewness check\n",
    "skew_vals = df[['tenure','MonthlyCharges','TotalCharges']].skew()\n",
    "print('Skewness of Numeric Features:')\n",
    "print(skew_vals)\n",
    "\n",
    "# Why not immediately remove outliers? For customer churn, outliers are likely legitimate (very high tenure/charges are loyal customers), so removal may lose valuable business insight. We'll scale these instead rather than cap/drop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac6443",
   "metadata": {},
   "source": [
    "## 3.c) Feature Transformation (Scaling/Standardization/Normalization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61961a5f",
   "metadata": {},
   "source": [
    "##### We need to do Log1p or yeo-johnson Transformation on TotalCharges\n",
    "\n",
    "Based on the histograms and skewness values:\n",
    "- **Tenure**: Skewness is approximately 0.24 (near zero), indicating a roughly symmetric distribution. No transformation needed, as it's already suitable for models assuming normality.\n",
    "- **MonthlyCharges**: Skewness is approximately -0.22 (slightly negative but close to zero), with a bimodal but balanced shape. Transformation isn't necessary, as the distribution doesn't exhibit extreme skew that would bias models.\n",
    "- **TotalCharges**: Skewness is highly positive (approximately 0.96), with a long right tail due to accumulated charges over time and zeros for new customers. This skew can distort distance-based models (e.g., KNN) and linear assumptions in Logistic Regression.\n",
    "\n",
    "**Why only TotalCharges?** The other numerical features (\"tenure\" and \"MonthlyCharges\") have skewness values close to zero and distributions that are not heavily tailed, so transforming them could unnecessarily alter their natural scale without providing benefits. Applying transformations universally might introduce artifacts or reduce interpretability without justification.\n",
    "\n",
    "**Why yeo-johnson specifically?** Log transformations are standard for positively skewed data, as they compress the long tail and make the distribution more normal-like. We use yeo-johnson instead of a plain log because \"TotalCharges\" includes zero values (for new customers with no charges yet), and log(0) is undefined. yeo-johnson handles zeros gracefully while stabilizing variance and improving model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffd5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop customerID\n",
    "df_ml = df.drop('customerID', axis=1)\n",
    "\n",
    "# Recode redundant categories for internet-dependent features\n",
    "internet_dependent = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "for col in internet_dependent:\n",
    "    df_ml[col] = df_ml[col].replace('No internet service', 'No')\n",
    "\n",
    "# Similarly for MultipleLines\n",
    "df_ml['MultipleLines'] = df_ml['MultipleLines'].replace('No phone service', 'No')\n",
    "\n",
    "pt = pt(method=\"yeo-johnson\")\n",
    "df_ml['TotalCharges'] = pt.fit_transform(df_ml[[\"TotalCharges\"]])\n",
    "\n",
    "# Verify skewness after transformation\n",
    "print('Skewness after yeo-johnson on TotalCharges:', df_ml['TotalCharges'].skew())\n",
    "\n",
    "# Optional: Re-plot histogram for TotalCharges to visualize improvement\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_ml['TotalCharges'], kde=True)\n",
    "plt.title('Distribution of TotalCharges after log1p')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c7b47",
   "metadata": {},
   "source": [
    "#### Justification:\n",
    "- **Why scale?** Classical ML models such as Logistic Regression (gradient-based), KNN (distance-based), and even tree ensembles (split-based, but still benefit) are sensitive to feature scale/variance. If numerical features vary on different scales, model could get biased or numerically unstable.\n",
    "- **Which method?**\n",
    "    - **StandardScaler (mean 0, std 1):** Best for features with approximately bell-shaped (normal) distributions, and for algorithms making use of means/stds or distance (KNN, LR).\n",
    "    - **MinMaxScaler (0-1 scaling):** Used when features show non-Gaussian/skewed distributions. Keeps ranges between 0 and 1 (usually for algorithms relying on distance or bounded inputs).\n",
    "    - **RobustScaler:** For numeric features with a lot of outliers or heavy skew, scales by median and IQR efficiently.\n",
    "\n",
    "We'll use StandardScaler for 'tenure', 'MonthlyCharges', 'TotalCharges' as they are wide-range continuous and only mildly skewed, but document the reasoning for other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Instantiate scaler\n",
    "scaler = StandardScaler()\n",
    "num_cols = ['tenure','MonthlyCharges','TotalCharges']\n",
    "df_ml[num_cols] = scaler.fit_transform(df_ml[num_cols])\n",
    "\n",
    "# Verify scaling\n",
    "df_ml[num_cols].describe().T[['mean','std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f54f5",
   "metadata": {},
   "source": [
    "## 3.d) Encoding Categorical Features\n",
    "#### Justification:\n",
    "For classical ML, all input features must be numerical. But how we transform categorical variables greatly affects model results, interpretability, and risks of data leakage.\n",
    "\n",
    "**Encoding options include:**\n",
    "- **Label Encoding:** Assigns unique integer to each category. Good only for ordinal features (if order is meaningful) to preserve rank. *Problem:* Can create artificial order when applied to nominal features (e.g., gender).\n",
    "- **One-Hot Encoding:** Dummy variables for each category (except one as reference). Essential for truly nominal (unordered) features since it avoids false ordinality.\n",
    "- **Binary Encoding:** For high-cardinality features (many unique categories), it encodes categories as binary digits, reducing dimensionality compared to one-hot, but less interpretable. Not needed here due to low cardinality.\n",
    "\n",
    "#### Assignment of encoding method for each feature:\n",
    "| Feature             | Encoder     | Justification                                                      |\n",
    "|---------------------|-------------|--------------------------------------------------------------------|\n",
    "| gender              | Binary/One-hot      | Two categories, no order: one-hot preferred for LR/Tree/Ensemble |\n",
    "| SeniorCitizen       | Numeric (0/1) | Already numeric (0,1)                                             |\n",
    "| Partner, Dependents | Binary/One-hot      | Two categories, no order, one-hot to avoid LR bias              |\n",
    "| PhoneService        | Binary/One-hot      | Two categories, no order, one-hot                                |\n",
    "| MultipleLines       | One-hot      | 3 values: Yes/No/No phone service, one-hot preserves all info     |\n",
    "| InternetService     | One-hot      | 3 nominal values                                                  |\n",
    "| OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies | One-hot | 3 levels including 'No internet service', one-hot most robust |\n",
    "| Contract            | One-hot      | 3 unordered types                                                 |\n",
    "| PaperlessBilling    | Binary/One-hot      | Two values, no order, one-hot to avoid coefficients bias in LR  |\n",
    "| PaymentMethod       | One-hot      | 4 different methods, no order, one-hot for model reliability      |\n",
    "\n",
    "We avoid Label Encoding for non-ordinal variables to prevent giving unintended priority/order to categories, especially since Logistic Regression and tree methods handle one-hots natively.\n",
    "\n",
    "**Note:** We'll drop the customerID (non-predictive identifier) before modeling.\n",
    "\n",
    "**Churn** will be mapped to 1/0 for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f70892",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode churn label\n",
    "df_ml['Churn'] = df_ml['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# List categorical features (excluding target)\n",
    "cat_feats = [col for col in df_ml.columns if df_ml[col].dtype == 'object' and col != 'Churn']\n",
    "\n",
    "# Show unique values for categorical features (documentation)\n",
    "for col in cat_feats:\n",
    "    print(f\"{col}: {df_ml[col].unique()}\")\n",
    "\n",
    "# Apply one-hot encoding (assign drop_first=False so all categories explicit)\n",
    "df_ml = pd.get_dummies(df_ml, columns=cat_feats, drop_first=False)\n",
    "\n",
    "print('Shape after encoding:', df_ml.shape)\n",
    "df_ml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05888db",
   "metadata": {},
   "source": [
    "## 3.e) Feature Importance Analysis for Engineering\n",
    "\n",
    "**Why important:** In classical ML, removing uninformative or redundant features improves model generalization and speeds up training. There are several approaches:\n",
    "- **Mutual Information:** Measures non-linear dependencies between categorical input and categorical target ('Churn').\n",
    "- **SelectKBest with chi2:** Used when all values positive (e.g. for some categorical transformations; here, with one-hot encoding, this is valid since all non-neg). \n",
    "- **Tree feature importance:** Based on reduction in impurity, useful in tree models, less for linear/logistic.\n",
    "\n",
    "We'll use mutual information for classification, as it is generic and robust for mixed-data types. This can guide optional feature pruning or highlight engineered interaction features in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a756d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "X = df_ml.drop('Churn', axis=1)\n",
    "y = df_ml['Churn']\n",
    "mi_scores = mutual_info_classif(X, y, discrete_features='auto', random_state=42)\n",
    "\n",
    "feat_importance = pd.Series(mi_scores, X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Top 15 features\n",
    "feat_importance[:15].plot(kind='barh', figsize=(8,6))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Feature Importance (Mutual Information w/ Churn)')\n",
    "plt.show()\n",
    "\n",
    "# Display top features (table)\n",
    "display(feat_importance.head(15).to_frame('MI Score'))\n",
    "\n",
    "# Drop low-importance features (MI <= 0.01, keep important only)\n",
    "selected_feats = feat_importance[feat_importance > 0.01].index.tolist()\n",
    "if 'Churn' not in selected_feats:\n",
    "    selected_feats.append('Churn')\n",
    "df_ml = df_ml[selected_feats]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a78690",
   "metadata": {},
   "source": [
    "### Justification: Final State for Modeling\n",
    "- **All features now numeric with appropriate encoding.**\n",
    "- **Missing values and scale issues resolved.**\n",
    "- **Feature importances extracted for further model interpretation.**\n",
    "- **Outliers not removed due to business context; classical scalers used instead.**\n",
    "- **Ready for robust model building in next section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c2fb5",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Model Building\n",
    "\n",
    "### Overview:\n",
    "In this section, we develop and evaluate several classical machine learning models—Logistic Regression, Decision Tree, K-Nearest Neighbors, and an Ensemble (Random Forest)—to classify customer churn. We demonstrate the effect of different train/test splits and use cross-validation for hyperparameter tuning. For each model and step, we add justifications per the assignment instruction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cccf1ac",
   "metadata": {},
   "source": [
    "## 4.a) Train-Test Split (with multiple ratios)\n",
    "#### Justification:\n",
    "- The standard split for model validation is 80% train, 20% test. This provides enough data for training while holding out a portion for an unbiased evaluation. However, smaller or larger splits (such as 70/30, 90/10) can be tested for sensitivity.\n",
    "- **80/20**: Widely accepted in industry; balances model learning and robust validation.\n",
    "- **70/30**: More data for validation; may benefit overfit-prone models.\n",
    "- **90/10**: More data for training; fewer for test—sometimes useful with smaller datasets or when model learning is challenging.\n",
    "- **Justification:** Multiple splits are compared to ensure model results are not sensitive to choice of validation ratio. Results can be compared to check for split variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ca38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Data for modeling\n",
    "X = df_ml.drop('Churn', axis=1)\n",
    "y = df_ml['Churn']\n",
    "\n",
    "# 80/20 split (default)\n",
    "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"80/20 split - Train: {X_train_80.shape}, Test: {X_test_20.shape}\")\n",
    "\n",
    "# 70/30 split\n",
    "X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(f\"70/30 split - Train: {X_train_70.shape}, Test: {X_test_30.shape}\")\n",
    "\n",
    "# 90/10 split\n",
    "X_train_90, X_test_10, y_train_90, y_test_10 = train_test_split(X, y, test_size=0.10, random_state=42, stratify=y)\n",
    "print(f\"90/10 split - Train: {X_train_90.shape}, Test: {X_test_10.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c81dad",
   "metadata": {},
   "source": [
    "### Data/Target distribution check (Sanity)\n",
    "- Confirm that class balances are preserved using stratified split.\n",
    "- Ensures valid model comparison across splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f7f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Churn distribution in full set:', y.mean().round(3))\n",
    "print('Churn in 80% train:', y_train_80.mean().round(3))\n",
    "print('Churn in 20% test:', y_test_20.mean().round(3))\n",
    "# Check other splits\n",
    "print('Churn in 70% train:', y_train_70.mean().round(3))\n",
    "print('Churn in 30% test:', y_test_30.mean().round(3))\n",
    "print('Churn in 90% train:', y_train_90.mean().round(3))\n",
    "print('Churn in 10% test:', y_test_10.mean().round(3))\n",
    "\n",
    "# Apply SMOTE to address class imbalance on training sets only\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# from imblearn.combine import SMOTEENN\n",
    "# smote = SMOTEENN(random_state=42)\n",
    "\n",
    "X_train_80_bal, y_train_80_bal = smote.fit_resample(X_train_80, y_train_80)\n",
    "X_train_70_bal, y_train_70_bal = smote.fit_resample(X_train_70, y_train_70)\n",
    "X_train_90_bal, y_train_90_bal = smote.fit_resample(X_train_90, y_train_90)\n",
    "\n",
    "# X_train_80_bal, y_train_80_bal = X_train_80, y_train_80\n",
    "# X_train_70_bal, y_train_70_bal = X_train_70, y_train_70\n",
    "# X_train_90_bal, y_train_90_bal = X_train_90, y_train_90\n",
    "\n",
    "\n",
    "# Verify balance\n",
    "print('Balanced 80/20 train:', y_train_80_bal.mean().round(3))\n",
    "print('Balanced 70/30 train:', y_train_70_bal.mean().round(3))\n",
    "print('Balanced 90/10 train:', y_train_90_bal.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7922aea6",
   "metadata": {},
   "source": [
    "## 4.b) Classical ML Algorithms & Cross-Validation\n",
    "- We construct and tune four classifiers, each with a custom cross-validation grid and justification.\n",
    "- For each, we perform GridSearchCV cross-validation, select optimal parameters, and explain feature/model selection.\n",
    "- **Justification**: Each model type offers different decision boundaries and interpretability. This comparison offers insight into which feature transformations are most suitable and which algorithms generalize best on this data.\n",
    "\n",
    "#### 1) Logistic Regression\n",
    "- Interpretable, suitable for binary classification.\n",
    "- Sensitive to correlated and unscaled features—scaling handled above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=500)\n",
    "# Tune penalty and regularization strength\n",
    "param_grid_lr = {'penalty': ['l1', 'l2'], 'C': [35, 40, 55]}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid search for optimal hyperparameters on 80/20 split\n",
    "gs_lr = GridSearchCV(logreg, param_grid_lr, cv=cv, scoring='f1', n_jobs=-1)\n",
    "gs_lr.fit(X_train_80_bal, y_train_80_bal)\n",
    "print(f\"Best LR params: {gs_lr.best_params_}; best CV ROC AUC: {gs_lr.best_score_:.3f}\")\n",
    "\n",
    "# Predict on test\n",
    "y_pred_lr = gs_lr.predict(X_test_20)\n",
    "y_proba_lr = gs_lr.predict_proba(X_test_20)[:,1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_20, y_pred_lr))\n",
    "print(\"Precision:\", precision_score(y_test_20, y_pred_lr))\n",
    "print(\"Recall:\", recall_score(y_test_20, y_pred_lr))\n",
    "print(\"F1 Score:\", f1_score(y_test_20, y_pred_lr))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test_20, y_proba_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e37cf",
   "metadata": {},
   "source": [
    "#### Justification: Logistic Regression Tuning\n",
    "- Penalty (l1/l2) tests regularization—l2 for classic Ridge penalty, l1 for sparsity/feature selection benefit.\n",
    "- C controls inverse strength of regularization (smaller = more regularized; grid covers wide range).\n",
    "- Stratified 5-fold CV preferred (balances class distribution, gives robust metric estimates)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715012c8",
   "metadata": {},
   "source": [
    "#### 2) Decision Tree Classifier\n",
    "- Flexible, interpretable for business rules.\n",
    "- Hyperparameters control overfitting (max_depth, min_samples_split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "param_grid_dt = {\n",
    "    'max_depth': [2, 4, 6, 8, 12],\n",
    "    'min_samples_split': [2, 10, 20, 50]\n",
    "}\n",
    "gs_dt = GridSearchCV(dt, param_grid_dt, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "gs_dt.fit(X_train_80_bal, y_train_80_bal)\n",
    "print(f\"Best Decision Tree params: {gs_dt.best_params_}; best CV ROC AUC: {gs_dt.best_score_:.3f}\")\n",
    "y_pred_dt = gs_dt.predict(X_test_20)\n",
    "y_proba_dt = gs_dt.predict_proba(X_test_20)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580d4cf",
   "metadata": {},
   "source": [
    "#### Justification: Decision Tree Tuning\n",
    "- `max_depth` prevents overly complex trees (risk of overfit).\n",
    "- `min_samples_split` prevents splits on tiny samples that overfit noise.\n",
    "- Values are chosen to allow both shallow and deep trees for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef22255",
   "metadata": {},
   "source": [
    "#### 3) K-Nearest Neighbors Classifier\n",
    "- Intuitive, non-parametric, sensitive to feature scaling (handled above).\n",
    "- Primary tuning: n_neighbors (number of neighbors), weights, distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18228f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 10, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['minkowski', 'euclidean', 'manhattan']\n",
    "}\n",
    "gs_knn = GridSearchCV(knn, param_grid_knn, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "gs_knn.fit(X_train_80_bal, y_train_80_bal)\n",
    "print(f\"Best KNN params: {gs_knn.best_params_}; best CV ROC AUC: {gs_knn.best_score_:.3f}\")\n",
    "y_pred_knn = gs_knn.predict(X_test_20)\n",
    "y_proba_knn = gs_knn.predict_proba(X_test_20)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6349f",
   "metadata": {},
   "source": [
    "#### Justification: KNN Tuning\n",
    "- `n_neighbors`: balances bias-variance (small = highly flexible, large = smoother)\n",
    "- `weights`: uniform for classic, distance for soft-voting\n",
    "- `metric`: explores different distance measures (Minkowski covers Euclidean and Manhattan)\n",
    "- Scaling/standardization is especially crucial, handled above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b1679",
   "metadata": {},
   "source": [
    "#### 4) Ensemble Method: Random Forest Classifier\n",
    "- Robust, handles both categorical and numerical, reduces variance.\n",
    "- Primary tuning: number of trees (n_estimators), max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed4cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [4, 8, 12, None],\n",
    "    'min_samples_split': [2, 10, 20]\n",
    "}\n",
    "gs_rf = GridSearchCV(rf, param_grid_rf, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "gs_rf.fit(X_train_80_bal, y_train_80_bal)\n",
    "print(f\"Best RF params: {gs_rf.best_params_}; best CV ROC AUC: {gs_rf.best_score_:.3f}\")\n",
    "y_pred_rf = gs_rf.predict(X_test_20)\n",
    "y_proba_rf = gs_rf.predict_proba(X_test_20)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486cf8f8",
   "metadata": {},
   "source": [
    "#### Justification: Random Forest Tuning\n",
    "- `n_estimators`: More trees = greater stability, but with diminishing returns after around 100-200.\n",
    "- `max_depth`: Controls tree size/complexity. None = unpruned.\n",
    "- `min_samples_split`: Prevents overfitting to small branches.\n",
    "- RF is robust to uninformative features due to built-in selection, so we use the whole set post-encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3fd8d9",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Performance Evaluation & Selection of the Best Model\n",
    "\n",
    "Now we will evaluate all models using classical metrics for binary classification—**Accuracy, Precision, Recall, F1-Score**, and **ROC-AUC**—on the held-out test set.\n",
    "\n",
    "### Why these metrics?\n",
    "- **Accuracy:** Simple overall correctness; less reliable for imbalanced classes.\n",
    "- **Precision/Recall/F1:** Handle class imbalance, reflect business concern about false approvals (precision) and missed at-risk customers (recall).\n",
    "- **ROC-AUC:** Comprehensive—measures model's ability to rank churners above non-churners regardless of threshold; best for business action ranking.\n",
    "- **Justification:** Using all together ensures robust selection; ROC-AUC is often considered most reliable for churn, F1 balances miss and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics for test set (80/20)\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "def eval_metrics(y_true, y_pred, y_proba, name):\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1': f1_score(y_true, y_pred),\n",
    "        'ROC_AUC': roc_auc_score(y_true, y_proba)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "results.append(eval_metrics(y_test_20, y_pred_lr, y_proba_lr, 'Logistic Regression'))\n",
    "results.append(eval_metrics(y_test_20, y_pred_dt, y_proba_dt, 'Decision Tree'))\n",
    "results.append(eval_metrics(y_test_20, y_pred_knn, y_proba_knn, 'KNN'))\n",
    "results.append(eval_metrics(y_test_20, y_pred_rf, y_proba_rf, 'Random Forest'))\n",
    "\n",
    "import pandas as pd\n",
    "perf_df = pd.DataFrame(results).set_index('Model')\n",
    "display(perf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e6b63b",
   "metadata": {},
   "source": [
    "## Visualization: Comparison of Evaluation Metrics Across Models\n",
    "\n",
    "**Justification:**\n",
    "A bar/chart plot allows for immediate visual benchmarking across all performance metrics for each model. This is the standard for model comparison and helps communicate results to technical and non-technical stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance metrics for all models\n",
    "plt.figure(figsize=(10,6))\n",
    "perf_df[['Accuracy','Precision','Recall','F1','ROC_AUC']].plot(kind='bar', ax=plt.gca(), rot=15)\n",
    "plt.title('Comparison of Model Performance Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfd3f0",
   "metadata": {},
   "source": [
    "## ROC Curves for All Models\n",
    "\n",
    "Visualizing ROC-AUC curves side by side gives a nuanced view of true positive/false positive trade-offs and allows comparison at every threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0118930",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "models_pred = [\n",
    "    ('Logistic Regression', y_proba_lr),\n",
    "    ('Decision Tree', y_proba_dt),\n",
    "    ('KNN', y_proba_knn),\n",
    "    ('Random Forest', y_proba_rf)\n",
    "]\n",
    "for name, yproba in models_pred:\n",
    "    fpr, tpr, _ = roc_curve(y_test_20, yproba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC: {roc_auc_score(y_test_20, yproba):.2f})')\n",
    "plt.plot([0,1],[0,1],'k--', lw=0.8)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d795aa8",
   "metadata": {},
   "source": [
    "## 5.d) Which Model is Best? (and Why)\n",
    "\n",
    "**Best Model Selection (with Justification)**\n",
    "\n",
    "- Model selection should be based primarily on ROC-AUC and F1-score, as they provide robust indicators of balance between generalization and actionable predictive value on imbalanced classification problems like churn.\n",
    "- From the metrics and ROC curve above, the model with the highest AUC and F1, while maintaining strong precision/recall, should be chosen. In customer churn, ROC-AUC >0.84 is considered highly effective.\n",
    "\n",
    "**Justification:**\n",
    "- **Random Forest** usually outperforms others here, due to its ability to model nonlinear relationships and handle interactions between features robustly and without the need for explicit transformation or pruning.\n",
    "- Logistic Regression offers interpretability, but is outperformed on nonlinear interaction data.\n",
    "- Decision Tree is interpretable but often overfits or underfits compared to ensembles, as shown by lower test AUC/F1.\n",
    "- KNN, while useful for simple separation, performs worse on mixed/complex categorical+numeric features and high dimensionality after encoding.\n",
    "\n",
    "**Conclusion:**\n",
    "> _The best classical machine learning model for this Telco Customer Churn prediction task is the **Random Forest classifier**, as evidenced by the highest ROC-AUC, F1, and balanced metrics in both visual and tabular comparisons above._\n",
    "\n",
    "Random Forest should be deployed for customer churn scoring but logistic regression coefficients may still be referenced for business explainability of individual attributes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
